{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03407c7-1cf0-442e-a75e-747873293040",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We will need the RBCPath type from the rbclib package to load data from the RBC.\n",
    "from rbclib import RBCPath\n",
    "\n",
    "# We'll also want to load some data directly from the filesystem.\n",
    "from pathlib import Path\n",
    "\n",
    "# We'll want to load/process some of the data using pandas and numpy.\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce94a18c-ebfe-4f29-a808-80b7ead1b683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This path refers to the repo github.com:ReproBrainChart/PNC_FreeSurfer;\n",
    "# Subject 1000393599's directory is used as an example.\n",
    "subject_id = 1000393599\n",
    "# To browse the repo, use this link:\n",
    "# https://github.com/ReproBrainChart/PNC_FreeSurfer/tree/main\n",
    "sub_path = RBCPath(f'rbc://PNC_FreeSurfer/freesurfer/sub-{subject_id}')\n",
    "\n",
    "# This path refers to a directory:\n",
    "assert sub_path.is_dir()\n",
    "\n",
    "# Print each file in the directory:\n",
    "for file in sub_path.iterdir():\n",
    "    print(repr(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e45a8a-597a-4fd6-a19d-125db36df02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can construct new paths by using the `/` operator. This is identical to\n",
    "# how paths are constructed in the `pathlib` module.\n",
    "stats_filepath = sub_path / f'sub-{subject_id}_regionsurfacestats.tsv'\n",
    "\n",
    "# Use pandas to read in the TSV file then display it:\n",
    "\n",
    "print(f\"Loading {stats_filepath} ...\")\n",
    "with stats_filepath.open('r') as f:\n",
    "    data = pd.read_csv(f, sep='\\t')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d861be3-fe99-4f6d-a41f-607668134594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Participant meta-data is generally located in the BIDS repository for each\n",
    "# study:\n",
    "rbcdata_path = Path('/home/jovyan/shared/data/RBC')\n",
    "train_filepath = rbcdata_path / 'train_participants.tsv'\n",
    "test_filepath = rbcdata_path / 'test_participants.tsv'\n",
    "\n",
    "# Load the PNC participants TSV files...\n",
    "with train_filepath.open('r') as f:\n",
    "    train_data = pd.read_csv(f, sep='\\t')\n",
    "with test_filepath.open('r') as f:\n",
    "    test_data = pd.read_csv(f, sep='\\t')\n",
    "\n",
    "# We can also concatenate the two datasets into a single dataset of all\n",
    "# study participants:\n",
    "all_data = pd.concat([train_data, test_data])\n",
    "\n",
    "# Display the full dataframe:\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029e3fc5-a5be-430e-b996-770af13c8070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ec7e48-7722-4868-876f-d5eb1a842dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eda_pipeline(data):\n",
    "    # Drop NA values for parent_1_education\n",
    "    data = data.dropna(subset=['parent_1_education'])\n",
    "    # Fill NA values for bmi and parent_2_education\n",
    "    # Fill bmi with median - slight right skew to original distribution\n",
    "    data['bmi'] = data['bmi'].fillna(data['bmi'].median())\n",
    "    # Fill parent_2_education with median - education is ordinal\n",
    "\n",
    "    ordinal_map_p_ed_2 = {\n",
    "    'No/incomplete primary': 0,\n",
    "    'Complete primary': 1,\n",
    "    'Complete secondary': 2,\n",
    "    'Complete tertiary': 3\n",
    "    }\n",
    "    data['ordinal_p_ed_2'] = data['parent_2_education'].map(ordinal_map_p_ed_2)\n",
    "    median_code = int(data['ordinal_p_ed_2'].median())\n",
    "    # Reverse mapping\n",
    "    rev_map = {v: k for k, v in ordinal_map_p_ed_2.items()}\n",
    "    median_category = rev_map[median_code]\n",
    "    data['parent_2_education'] = data['parent_2_education'].fillna(median_category)\n",
    "    data = data.drop(columns = ['ordinal_p_ed_2'])\n",
    "    # Encode categorical binary variables (Sex, Handedness, Ethnicity)\n",
    "    # Example placeholders: 'sex' → Male=1, Female=0; 'handedness' → Right=1, Left=0; 'ethnicity' → Majority=1, Minority=0\n",
    "    \n",
    "    binary_encodings = {\n",
    "        'sex': {'Male': 1, 'Female': 0},\n",
    "        'ethnicity': {'not Hispanic or Latino': 1, 'Hispanic or Latino': 0}\n",
    "    }\n",
    "    \n",
    "    for col, mapping in binary_encodings.items():\n",
    "        if col in data.columns:\n",
    "            data[col] = data[col].map(mapping)\n",
    "    \n",
    "    # Standardize continuous variables (Age & BMI)\n",
    "    scaler = StandardScaler()\n",
    "    data[['age_std', 'bmi_std']] = scaler.fit_transform(data[['age', 'bmi']])\n",
    "    \n",
    "    # One-Hot Encode Multicategory Features: parent_2_education, participant_education, race\n",
    "    categorical_cols = ['parent_1_education','parent_2_education', 'participant_education', 'race', 'handedness']\n",
    "        \n",
    "    # Apply one-hot encoding with pandas\n",
    "    data = pd.get_dummies(\n",
    "        data,\n",
    "        columns=categorical_cols,\n",
    "        prefix=categorical_cols,\n",
    "        # keep all categories - might be good for more complex models\n",
    "        drop_first=False  # keep all categories\n",
    "    )\n",
    "    # Convert all resulting dummy columns to int (0/1)\n",
    "    dummy_cols = [col for col in data.columns if any(col.startswith(c + '_') for c in categorical_cols)]\n",
    "    data[dummy_cols] = data[dummy_cols].astype(int)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8715ae83-04fa-441d-b53e-3e3eaeea2d25",
   "metadata": {},
   "source": [
    "Example: using train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f1df7b-aebb-4065-a9b8-01ba40296afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "td3 = eda_pipeline(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1718a52a-cf64-4b1b-94d8-2ade585b837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(td3.shape)\n",
    "print(td3.columns)\n",
    "print(td3.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1ffe89-3f26-43a2-add6-af562d61a884",
   "metadata": {},
   "outputs": [],
   "source": [
    "td3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601c3998-277d-4305-8480-a42fd9edffe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521099ba-7749-4748-bc94-32f96a661ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c5b8c9-a5cb-49c4-9dd0-7d7286fd46b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656be70d-0423-4258-8573-b524b9995504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
